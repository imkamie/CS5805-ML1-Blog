---
execute:
  echo: fenced
title: "Classification"
title-block-banner: false
author: Kamila Nurkhametova
---

Classification is a fundamental task in supervised machine learning, aiming to assign predefined labels or categories to input data based on their features. During the training phase, the algorithm learns patterns and relationships between input features and labels, enabling it to predict labels for new, unseen data. Let's delve into key concepts related to classification:

## Key Concepts

### Classes or Labels
These are the predefined categories that the algorithm assigns to input data. Binary classification involves two classes (e.g., spam or not spam), while multiclass classification deals with more than two classes.

### Features
Features are characteristics or attributes of input data used by the algorithm to make predictions. They can be numerical or categorical and are represented as input variables.

### Training Data
The labeled dataset used to train the classification model, consisting of examples with input features and corresponding output labels.

### Model
The learned representation of patterns and relationships in the training data, capable of making predictions on new, unseen data.

### Prediction
Once trained, the model predicts class labels for new instances based on their input features.

## Common Classification Algorithms

Various algorithms are employed for classification, each suited to different scenarios. Some common ones include logistic regression, decision trees, support vector machines, k-nearest neighbors, and neural networks. The choice depends on factors like data nature, complexity of relationships, and desired model interpretability.

## Classification in Action

Now, let's implement a simple classification example using the Iris dataset and the K-Nearest Neighbors (KNN) algorithm.

```{python}
# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.decomposition import PCA

# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize features by removing the mean and scaling to unit variance
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize and train the K-Nearest Neighbors classifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# Make predictions on the test set
y_pred = knn.predict(X_test)

# Calculate and print accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Visualize the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()

classes = iris.target_names
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes)
plt.yticks(tick_marks, classes)

plt.xlabel('Predicted Label')
plt.ylabel('True Label')

for i in range(len(classes)):
    for j in range(len(classes)):
        plt.text(j, i, format(conf_matrix[i, j], 'd'),
                 ha="center", va="center",
                 color="white" if conf_matrix[i, j] > conf_matrix.max() / 2. else "black")

plt.show()

# Visualize the first two principal components using PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

plt.figure(figsize=(8, 6))
for i in range(len(classes)):
    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], label=classes[i])

plt.title('PCA of Iris dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.show()

```

In this example, we use the Iris dataset, split it into training and testing sets, standardize features, and train a K-Nearest Neighbors classifier. We evaluate its performance using accuracy and visualize the confusion matrix and the first two principal components using PCA. The confusion matrix provides insights into the classifier's performance on different classes, while PCA visualization helps in understanding the data's structure in a lower-dimensional space.

## Conclusion

Classification is a powerful tool with applications ranging from spam detection to medical diagnosis. Understanding the underlying concepts and utilizing appropriate algorithms are essential for building accurate and effective models. Through continuous exploration and refinement, the field of classification continues to contribute significantly to the advancement of machine learning and data science.