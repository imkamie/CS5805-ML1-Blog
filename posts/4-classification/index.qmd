---
execute:
  echo: fenced
title: "Classification"
toc: true
toc-title: "Table of Contents"
title-block-banner: false
author: Kamila Nurkhametova
---

In machine learning, classification is a type of supervised learning task where the goal is to assign predefined labels or categories to input data based on its features. The input data, also known as instances or examples, is presented to the algorithm along with their corresponding labels during the training phase. The algorithm learns the patterns and relationships between the input features and the labels, enabling it to make predictions on new, unseen data.

The process of classification involves building a model that can generalize from the training data to accurately predict the labels of new instances. The trained model essentially forms a decision boundary or a set of rules that separate different classes in the feature space.

Here are some key concepts related to classification in machine learning:

**Classes or Labels:** These are the categories or groups that the algorithm aims to assign to input data. For binary classification, there are two classes (e.g., spam or not spam), while multiclass classification involves more than two classes.

**Features:** These are the characteristics or attributes of the input data that the algorithm uses to make predictions. Features can be numerical or categorical and are represented as input variables.

**Training Data:** This is the labeled dataset used to train the classification model. It consists of examples where each example has both input features and the corresponding correct output labels.

**Model:** The model is the learned representation of the patterns and relationships in the training data. It is capable of making predictions on new, unseen data.

**Prediction:** Once the model is trained, it can be used to predict the class labels of new instances based on their input features.

Common algorithms used for classification include logistic regression, decision trees, support vector machines, k-nearest neighbors, and neural networks. The choice of algorithm depends on factors such as the nature of the data, the complexity of the relationships between features and labels, and the desired interpretability of the model.

```{python}
# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.decomposition import PCA

# Load the Iris dataset
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize features by removing the mean and scaling to unit variance
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Initialize and train the K-Nearest Neighbors classifier
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# Make predictions on the test set
y_pred = knn.predict(X_test)

# Calculate and print accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Visualize the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Confusion Matrix')
plt.colorbar()

classes = iris.target_names
tick_marks = np.arange(len(classes))
plt.xticks(tick_marks, classes)
plt.yticks(tick_marks, classes)

plt.xlabel('Predicted Label')
plt.ylabel('True Label')

for i in range(len(classes)):
    for j in range(len(classes)):
        plt.text(j, i, format(conf_matrix[i, j], 'd'),
                 ha="center", va="center",
                 color="white" if conf_matrix[i, j] > conf_matrix.max() / 2. else "black")

plt.show()

# Visualize the first two principal components using PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

plt.figure(figsize=(8, 6))
for i in range(len(classes)):
    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], label=classes[i])

plt.title('PCA of Iris dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.show()

```