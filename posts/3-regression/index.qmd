---
execute:
  echo: fenced
title: "Linear and Nonlinear regression"
toc: true
toc-title: "Table of Contents"
title-block-banner: false
author: Kamila Nurkhametova
---


Linear and nonlinear regression are both techniques used in machine learning and statistics to model the relationship between independent variables (features) and a dependent variable (target). The primary difference between them lies in the nature of the relationship they can capture.

## Linear Regression:

**Nature of Relationship:**
   Linear regression assumes a linear relationship between the independent and dependent variables. This means that the relationship can be represented by a straight line.

**Equation:**
   The equation for a simple linear regression with one independent variable is: \(y = mx + b\), where \(y\) is the dependent variable, \(x\) is the independent variable, \(m\) is the slope, and \(b\) is the y-intercept.

**Optimization:**
   The model parameters (slope and intercept) are typically estimated using methods like Ordinary Least Squares (OLS) to minimize the sum of squared differences between the observed and predicted values.

<!-- **Example:**
   Predicting house prices based on the number of bedrooms. -->

## Nonlinear Regression:

**Nature of Relationship:**
   Nonlinear regression allows for more complex relationships between the independent and dependent variables. The relationship can take various forms, such as quadratic, exponential, logarithmic, or other nonlinear shapes.

**Equation:**
   The equation for a general nonlinear regression model is more complex and depends on the specific form of the relationship being modeled. For example, it could be \(y = a \cdot \log(x) + b\).

**Optimization:**
   Optimization techniques such as gradient descent or other nonlinear optimization methods are used to find the parameters that minimize the difference between the predicted and observed values.

<!-- **Example:** -->
   <!-- Modeling the growth of a population over time using an exponential function. -->

In practice, the choice between linear and nonlinear regression depends on the characteristics of the data and the underlying assumptions about the relationship being modeled. Data exploration and validation techniques are essential in determining the most appropriate approach.

```{python}
# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.datasets import fetch_california_housing

# Load the Boston Housing dataset
boston = fetch_california_housing()
X = boston.data
y = boston.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a linear regression model
model = LinearRegression()

# Train the model on the training set
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
slope, intercept = np.polyfit(y_test, y_pred, 1)
best_fit_line = np.poly1d([slope, intercept])
# Plotting the results
plt.scatter(y_test, y_pred)
plt.plot(y_test, best_fit_line(y_test), 'r-', lw=2)
plt.xlabel("Actual Prices")
plt.ylabel("Predicted Prices")
plt.title("Actual Prices vs Predicted Prices")
plt.show()

```