---
execute:
  echo: fenced
title: "Clustering"
title-block-banner: false
author: Kamila Nurkhametova
---

Clustering, a cornerstone of unsupervised machine learning, empowers the discovery of inherent patterns and relationships within data without pre-existing labels. This versatile technique finds applications in diverse domains, including data analysis, customer segmentation, recommender systems, search engines, image segmentation, semi-supervised learning, and dimensionality reduction.

## k-means: Partitioning Data with Centroids

The k-means algorithm, pioneered by Stuart Lloyd at Bell Labs in 1957, has evolved into a powerful clustering method. Although initially conceived for pulse-code modulation, its publication in 1982 marked its entry into broader usage. Sometimes referred to as the Lloydâ€“Forgy algorithm, its core principle involves partitioning data into k clusters based on similarity.

Before we delve into the implementation, let's import the necessary libraries for our clustering analysis.

```{python}
# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_wine
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
```

**Determining Optimal Number of Clusters with the Elbow Method:**
The Elbow Method is a technique used to find the optimal number of clusters in a dataset. It involves running k-means clustering on the dataset for a range of values of k (number of clusters) and plotting the sum of squared distances from each point to its assigned center. The "elbow" of the curve represents the point where adding more clusters does not significantly improve the fit.

```{python}
# Load the wine dataset
data = load_wine()
X = data.data

# Standardize the features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Determine the optimal number of clusters using the Elbow Method
distortions = []
for i in range(1, 10):
    kmeans = KMeans(n_clusters=i, n_init=10, random_state=42)
    kmeans.fit(X)
    distortions.append(kmeans.inertia_)

# Plot the Elbow Curve
plt.figure()
plt.plot(range(1, 10), distortions, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters')
plt.ylabel('Distortion')
plt.show()
```

The Elbow Method indicates that the optimal number of clusters for this dataset is 3.
We apply the k-means algorithm to the standardized dataset with three clusters. The resulting cluster labels are assigned to each data point. The scatter plot illustrates the data points in this reduced space, with each point colored according to its assigned cluster. The red markers represent the centroids of the clusters.

This visualization provides a clear representation of how the k-means algorithm has grouped the data into distinct clusters based on similarity.

```{python}
# Apply k-means clustering with k=3
kmeans = KMeans(n_clusters=3, n_init='auto')
kmeans.fit(X)
labels = kmeans.labels_

# Reduce data to two dimensions using PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Visualize the clusters and centroids in 2D
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')
centers_pca = pca.transform(kmeans.cluster_centers_)
plt.scatter(centers_pca[:, 0], centers_pca[:, 1], s=200, c='red')
plt.title('k-means Clustering')
plt.show()
```

## DBSCAN: Discovering Density-Based Clusters

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is an alternative approach to clustering that excels in identifying clusters with varying shapes and sizes.

Let's explore the application of DBSCAN to our dataset. Before running DBSCAN, we standardize the features using the `StandardScaler`. The hyperparameters `eps` and `min_samples` control the density-based clustering. `eps` defines the maximum distance between two samples for one to be considered as in the neighborhood of the other, and `min_samples` sets the minimum number of samples required to form a dense region.

```{python}
# Standardize the features for DBSCAN
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply DBSCAN clustering
dbscan = DBSCAN(eps=0.5, min_samples=5)
labels = dbscan.fit_predict(X_scaled)

# Reduce data to two dimensions using PCA
pca = PCA(n_components=2)
clustered_data_2d = pca.fit_transform(X_scaled)

# Visualize the clusters, highlighting noise points in red
plt.scatter(clustered_data_2d[:, 0], clustered_data_2d[:, 1], c=labels, cmap='viridis')
centers_pca = pca.transform(kmeans.cluster_centers_)
plt.scatter(centers_pca[:, 0], centers_pca[:, 1], s=200, c='red')

# Add labels and title
plt.title('DBSCAN Clustering')
plt.show()
```

Unlike k-means, DBSCAN can identify clusters of arbitrary shapes and is robust to outliers.

This visualization provides insights into the structure of the data, showcasing how DBSCAN has grouped points based on their density, thereby revealing clusters with varying shapes and sizes.

## Conclusion

k-means and DBSCAN serve as valuable tools in the clustering toolkit, offering distinct approaches to uncovering patterns and relationships in unlabeled data.
