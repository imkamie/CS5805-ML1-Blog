[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS5805 Machine Learning I Blog",
    "section": "",
    "text": "Anomaly/outlier detection\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nLinear and nonlinear regression\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/probability-theory/index.html",
    "href": "posts/probability-theory/index.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Machine learning often involves the use of probability theory and random variables to model uncertainty, make predictions, and make decisions. Let’s break down some key concepts in this context."
  },
  {
    "objectID": "posts/probability-theory/index.html#probability-theory",
    "href": "posts/probability-theory/index.html#probability-theory",
    "title": "Probability theory and random variables",
    "section": "Probability Theory",
    "text": "Probability Theory\nProbability: In machine learning, probability is used to quantify uncertainty. It is a measure of the likelihood that a particular event will occur. Probabilities range from 0 (impossible event) to 1 (certain event). In the context of machine learning, probabilities are often associated with outcomes of interest.\nConditional Probability: This is the probability of an event occurring given that another event has already occurred. In machine learning, understanding conditional probability is crucial, especially in Bayesian methods.\nBayesian Inference: It’s an approach to statistical inference where Bayes’ theorem is used to update the probability for a hypothesis as more evidence or information becomes available."
  },
  {
    "objectID": "posts/probability-theory/index.html#random-variables",
    "href": "posts/probability-theory/index.html#random-variables",
    "title": "Probability theory and random variables",
    "section": "Random Variables",
    "text": "Random Variables\nA random variable is a variable whose possible values are outcomes of a random phenomenon. In machine learning, random variables are used to model uncertainty in various processes.\nDiscrete Random Variables: These take on a countable number of distinct values. For example, the outcome of rolling a die is a discrete random variable.\nContinuous Random Variables: These can take any value within a range. Examples include the height of a person or the temperature at a given time."
  },
  {
    "objectID": "posts/probability-theory/index.html#probability-distributions",
    "href": "posts/probability-theory/index.html#probability-distributions",
    "title": "Probability theory and random variables",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nA probability distribution describes how the values of a random variable are distributed. In machine learning, probability distributions are used to model uncertainty in data and parameters.\nDiscrete Probability Distributions: Examples include the binomial distribution (for binary outcomes) and the Poisson distribution (for count data).\nContinuous Probability Distributions: Examples include the normal distribution (bell-shaped curve) and the exponential distribution."
  },
  {
    "objectID": "posts/probability-theory/index.html#expectation-and-variance",
    "href": "posts/probability-theory/index.html#expectation-and-variance",
    "title": "Probability theory and random variables",
    "section": "Expectation and Variance",
    "text": "Expectation and Variance\nExpectation (Mean): It is the average value of a random variable. In machine learning, it’s often used to summarize the central tendency of a distribution.\nVariance: It measures how spread out the values of a random variable are. It gives a sense of the dispersion of data."
  },
  {
    "objectID": "posts/probability-theory/index.html#joint-and-marginal-distributions",
    "href": "posts/probability-theory/index.html#joint-and-marginal-distributions",
    "title": "Probability theory and random variables",
    "section": "Joint and Marginal Distributions",
    "text": "Joint and Marginal Distributions\nJoint Distribution: Describes the probability of multiple random variables occurring simultaneously.\nMarginal Distribution: Describes the probability distribution of a subset of random variables without reference to the values of the other variables."
  },
  {
    "objectID": "posts/probability-theory/index.html#conclusion",
    "href": "posts/probability-theory/index.html#conclusion",
    "title": "Probability theory and random variables",
    "section": "Conclusion",
    "text": "Conclusion\nUnderstanding probability theory and random variables is crucial for various machine learning tasks, including classification, regression, and reinforcement learning. Probabilistic models provide a natural framework for dealing with uncertainty in real-world data."
  },
  {
    "objectID": "posts/probability-theory/index.html#basic-concepts",
    "href": "posts/probability-theory/index.html#basic-concepts",
    "title": "Probability theory and random variables",
    "section": "Basic concepts",
    "text": "Basic concepts\nBroadly speaking, probability theory is the mathematical study of uncertainty. It plays a central role in machine learning, as the design of learning algorithms often relies on probabilistic assumption of the data. This set of notes attempts to cover some basic probability theory that serves as a background for the class.\n\nProbability Theory\nIn machine learning, probability is used to quantify uncertainty. It is a measure of the likelihood that a particular event will occur. Probabilities range from 0 (impossible event) to 1 (certain event). In the context of machine learning, probabilities are often associated with outcomes of interest.\nConditional Probability. This is the probability of an event occurring given that another event has already occurred. In machine learning, understanding conditional probability is crucial, especially in Bayesian methods.\nBayesian Inference. It is an approach to statistical inference where Bayes’ theorem is used to update the probability for a hypothesis as more evidence or information becomes available.\n\n\nProbability Space\nWhen we speak about probability, we often refer to the probability of an event of uncertain nature taking place. For example, we speak about the probability of rain next Tuesday.\nFormally, a probability space is defined by the triple \\((\\Omega, \\mathcal{F}, P)\\), where\n\n\\(\\Omega\\) is the space of possible outcomes (or outcome space),\n\\(\\mathcal{F} \\subseteq 2^{\\Omega}\\) is the space of (measurable) events (or event space),\n\\(P\\) is the probability measure (or probability distribution) that maps an event \\(E \\in \\mathcal{F}\\) to a real value between 0 and 1 (think of \\(P\\) as a function).\n\n\n\nRandom Variables\nRandom variables play an important role in probability theory. The most important fact about random variables is that they are not variables. They are actually functions that map outcomes (in the outcome space) to real values. In terms of notation, we usually denote random variables by a capital letter\nIn machine learning, random variables are used to model uncertainty in various processes.\nDiscrete Random Variables. These take on a countable number of distinct values. For example, the outcome of rolling a die is a discrete random variable.\nContinuous Random Variables. These can take any value within a range. Examples include the height of a person or the temperature at a given time.\n\n\nProbability Distributions, Joint Distributions, and Marginal Distributions\nA probability distribution describes how the values of a random variable are distributed. In machine learning, probability distributions are used to model uncertainty in data and parameters.\nJoint Distribution describes the probability of multiple random variables occurring simultaneously.\nMarginal Distribution describes the probability distribution of a subset of random variables without reference to the values of the other variables.\nConditional distributions are one of the key tools in probability theory for reasoning about uncertainty. They specify the distribution of a random variable when the value of another random variable is known (or more generally, when some event is known to be true).\n\n\nBernoulli distribution\nThe Bernoulli distribution is one of the most basic distribution. A random variable distributed according to the Bernoulli distribution can take on two possible values, {0, 1}. It is often used to indicate whether a trail is successful or not.\n\n\nCode\n```{python}\n#| fig-cap: \"Bernoulli distribution\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import bernoulli\n\n# Set the mean rate for the Poisson distribution\np = 0.3\n\n# Generate data for a Bernoulli distribution\ndata = np.random.choice([0, 1], size=1000, p=[1 - p, p])\n\n# Create a histogram\nplt.hist(data, bins=[-0.5, 0.5, 1.5], density=True, alpha=0.6)\n\n# Plot the probability mass function (PMF) of the Bernoulli distribution\nx = [0, 1]\npmf_values = bernoulli.pmf(x, p)\nplt.vlines(x, 0, pmf_values, colors='k', linestyles='-', lw=2)\n\n# Add labels and a title\nplt.title('Bernoulli Distribution: $p$ = %.2f' % p)\nplt.xlabel('Outcome')\nplt.ylabel('Probability')\n\nplt.xticks([0, 1], ['Failure', 'Success'])\nplt.show()\n```\n\n\n\n\n\nBernoulli distribution\n\n\n\n\n\n\nPoisson distribution\nThe Poisson distribution is a very useful distribution that deals with the arrival of events. It measures probaiblity of the number of events happening over a fixed period of time, given a fixed average rate of occurrence, and that the events take place independently of the time since the last event.\n\n\nCode\n```{python}\n#| fig-cap: \"Poisson distribution\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import poisson\n\n# Set the mean rate for the Poisson distribution\nlam = 5\n\n# Generate data for a Poisson distribution\ndata = np.random.poisson(lam, 1000)\n\n# Create a histogram\nplt.hist(data, bins=50, density=True, alpha=0.6)\n\n# Plot the probability mass function (PMF) of the Poisson distribution\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = poisson.pmf(x, lam)\nplt.plot(x, p, 'k', linewidth=2)\n\n# Add labels and a title\nplt.title('Poisson Distribution: $\\lambda$ = %.2f' % lam)\nplt.xlabel('Number of Events')\nplt.ylabel('Probability')\n\nplt.show()\n```\n\n\n\n\n\nPoisson distribution\n\n\n\n\n\n\nGaussian distribution\nThe Gaussian distribution, also known as the normal distribution, is one of the most “versatile” distributions in probability theory, and appears in a wide variety of contexts. For example, it can be used to approximate the binomial distribution when the number of experiments is large, or the Poisson distribution when the average arrival rate is high. It is also related to the Law of Large Numbers. For many problems, we will also often assume that when noise in the system is Gaussian distributed. The list of applications is endless.\n\n\nCode\n```{python}\n#| fig-cap: \"Gussian distribution\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nmu = 0  # mean\nsigma = 1  # standard deviation\n\n# Generate data for a Gaussian distribution\ndata = np.random.normal(mu, sigma, 1000)\n\n# Create a histogram\nplt.hist(data, bins=50, density=True, alpha=0.6)\n\n# Plot the probability density function (PDF) of the Gaussian distribution\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mu, sigma)\nplt.plot(x, p, 'k')\n\n# Add labels and a title\nplt.title('Fit results: mu = %.2f,  std = %.2f' % (mu, sigma))\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\nplt.show()\n```\n\n\n\n\n\nGussian distribution"
  },
  {
    "objectID": "posts/probability-theory/index.html#defining-a-probability-distribution",
    "href": "posts/probability-theory/index.html#defining-a-probability-distribution",
    "title": "Probability theory and random variables",
    "section": "Defining a Probability Distribution",
    "text": "Defining a Probability Distribution\nThere are two classes of distribution that require seemingly different treatments. Namely, discrete distributions and continuous distributions.\n\nDiscrete Distribution\nBy a discrete distribution, we mean that the random variable of the underlying distribution can take on only finitely many different values (or that the outcome space is finite). Examples include the binomial distribution (for binary outcomes) and the Poisson distribution (for count data).\n\n\nContinuous Distribution\nBy a continuous distribution, we mean that the random variable of the underlying distribution can take on infinitely many different values (or that the outcome space is infinite). Examples include the normal distribution (bell-shaped curve) and the exponential distribution."
  },
  {
    "objectID": "posts/probability-theory/index.html#expectations-and-variance",
    "href": "posts/probability-theory/index.html#expectations-and-variance",
    "title": "Probability theory and random variables",
    "section": "Expectations and Variance",
    "text": "Expectations and Variance\nOne of the most common operations we perform on a random variable is to compute its expectation, also known as its mean, expected value, or first moment. The expectation is the average value of a random variable. In machine learning, it’s often used to summarize the central tendency of a distribution.\nVariance measures how spread out the values of a random variable are. It gives a sense of the dispersion of data. Sometimes it is also referred to as the second moment."
  }
]