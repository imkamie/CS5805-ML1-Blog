{
  "hash": "0a6bc54964ca0e829ae7702ed11344f2",
  "result": {
    "markdown": "---\nexecute:\n  echo: fenced\ntitle: \"Probability theory and random variables\"\ntitle-block-banner: false\nauthor: Kamila Nurkhametova\n---\n\n# Probability Theory and Random Variables\n\nMachine learning frequently relies on probability theory and random variables to model uncertainty, make predictions, and inform decision-making. In this post, we'll delve into key concepts in these domains.\n\n## Basic Concepts\n\n**Probability Theory:**\nProbability in machine learning quantifies uncertainty, representing the likelihood of an event occurring, ranging from 0 (impossible) to 1 (certain). This is pivotal in algorithm design, often associated with outcomes of interest.\n\n- **Conditional Probability:** Probability of an event given another has occurred. Crucial in Bayesian methods.\n- **Bayesian Inference:** Uses Bayes' theorem to update hypotheses' probability with new evidence.\n\n\n**Random Variables:**\nIn probability theory, these are functions mapping outcomes to real values, classifying uncertainty.\n\n- **Discrete Random Variables:** Have countable distinct values, e.g., die outcomes.\n- **Continuous Random Variables:** Can take any value within a range, like height or temperature.\n\n**Probability Distributions:**\nDescribe how a random variable's values are distributed. Key types include Joint, Marginal, and Conditional Distributions.\n\n## Bernoulli Distribution\n\nThe Bernoulli distribution models binary outcomes (0 or 1). Below is a Python example using `scipy.stats`.\n\n::: {.cell execution_count=1}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import bernoulli\n\n# Set the mean rate for the Poisson distribution\np = 0.3\n\n# Generate data for a Bernoulli distribution\ndata = np.random.choice([0, 1], size=1000, p=[1 - p, p])\n\n# Create a histogram\nplt.hist(data, bins=[-0.5, 0.5, 1.5], density=True, alpha=0.6)\n\n# Plot the probability mass function (PMF) of the Bernoulli distribution\nx = [0, 1]\npmf_values = bernoulli.pmf(x, p)\nplt.vlines(x, 0, pmf_values, colors='k', linestyles='-', lw=2)\n\n# Add labels and a title\nplt.title('Bernoulli Distribution: $p$ = %.2f' % p)\nplt.xlabel('Outcome')\nplt.ylabel('Probability')\n\nplt.xticks([0, 1], ['Failure', 'Success'])\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=589 height=450}\n:::\n:::\n\n\n## Poisson Distribution\n\nPoisson distribution models event occurrences over a fixed period. Example code:\n\n::: {.cell execution_count=2}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import poisson\n\n# Set the mean rate for the Poisson distribution\nlam = 5\n\n# Generate data for a Poisson distribution\ndata = np.random.poisson(lam, 1000)\n\n# Create a histogram\nplt.hist(data, bins=50, density=True, alpha=0.6)\n\n# Plot the probability mass function (PMF) of the Poisson distribution\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = poisson.pmf(x, lam)\nplt.plot(x, p, 'k', linewidth=2)\n\n# Add labels and a title\nplt.title('Poisson Distribution: $\\lambda$ = %.2f' % lam)\nplt.xlabel('Number of Events')\nplt.ylabel('Probability')\n\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=589 height=449}\n:::\n:::\n\n\n## Gaussian Distribution\n\nThe Gaussian distribution, or normal distribution, is versatile in probability theory. Example code:\n\n::: {.cell execution_count=3}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\nmu = 0  # mean\nsigma = 1  # standard deviation\n\n# Generate data for a Gaussian distribution\ndata = np.random.normal(mu, sigma, 1000)\n\n# Create a histogram\nplt.hist(data, bins=50, density=True, alpha=0.6)\n\n# Plot the probability density function (PDF) of the Gaussian distribution\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mu, sigma)\nplt.plot(x, p, 'k')\n\n# Add labels and a title\nplt.title('Fit results: mu = %.2f,  std = %.2f' % (mu, sigma))\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=597 height=449}\n:::\n:::\n\n\n## Defining a Probability Distribution\n\nTwo classes: Discrete and Continuous Distributions. Discrete involves finite values; Continuous involves infinite values.\n\n## Classification with Gaussian Naive Bayes\n\nA practical example of applying probability theory in machine learning using Gaussian Naive Bayes for Iris dataset classification.\n\n::: {.cell execution_count=4}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Standardize the features (mean=0 and variance=1)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Train a Gaussian Naive Bayes classifier\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = nb_classifier.predict(X_test)\n\n# Evaluate the accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy:.2f}')\n\n# Plot the confusion matrix using seaborn\nconf_matrix = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=iris.target_names, yticklabels=iris.target_names)\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 0.98\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=627 height=523}\n:::\n:::\n\n\n## Conclusion\n\nUnderstanding probability theory and random variables is pivotal in various machine learning tasks, providing a natural framework to handle uncertainty in real-world data.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}