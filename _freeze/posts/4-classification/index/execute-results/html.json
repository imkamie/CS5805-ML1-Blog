{
  "hash": "cd59b042b9bcdceda75ed4473d08b059",
  "result": {
    "markdown": "---\nexecute:\n  echo: fenced\ntitle: \"Classification\"\ntoc: true\ntoc-title: \"Table of Contents\"\ntitle-block-banner: false\nauthor: Kamila Nurkhametova\n---\n\nIn machine learning, classification is a type of supervised learning task where the goal is to assign predefined labels or categories to input data based on its features. The input data, also known as instances or examples, is presented to the algorithm along with their corresponding labels during the training phase. The algorithm learns the patterns and relationships between the input features and the labels, enabling it to make predictions on new, unseen data.\n\nThe process of classification involves building a model that can generalize from the training data to accurately predict the labels of new instances. The trained model essentially forms a decision boundary or a set of rules that separate different classes in the feature space.\n\nHere are some key concepts related to classification in machine learning:\n\n**Classes or Labels:** These are the categories or groups that the algorithm aims to assign to input data. For binary classification, there are two classes (e.g., spam or not spam), while multiclass classification involves more than two classes.\n\n**Features:** These are the characteristics or attributes of the input data that the algorithm uses to make predictions. Features can be numerical or categorical and are represented as input variables.\n\n**Training Data:** This is the labeled dataset used to train the classification model. It consists of examples where each example has both input features and the corresponding correct output labels.\n\n**Model:** The model is the learned representation of the patterns and relationships in the training data. It is capable of making predictions on new, unseen data.\n\n**Prediction:** Once the model is trained, it can be used to predict the class labels of new instances based on their input features.\n\nCommon algorithms used for classification include logistic regression, decision trees, support vector machines, k-nearest neighbors, and neural networks. The choice of algorithm depends on factors such as the nature of the data, the complexity of the relationships between features and labels, and the desired interpretability of the model.\n\n::: {.cell execution_count=1}\n```` { .cell-code}\n```{{python}}\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.decomposition import PCA\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Standardize features by removing the mean and scaling to unit variance\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Initialize and train the K-Nearest Neighbors classifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = knn.predict(X_test)\n\n# Calculate and print accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Visualize the confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8, 6))\nplt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\nclasses = iris.target_names\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes)\nplt.yticks(tick_marks, classes)\n\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\n\nfor i in range(len(classes)):\n    for j in range(len(classes)):\n        plt.text(j, i, format(conf_matrix[i, j], 'd'),\n                 ha=\"center\", va=\"center\",\n                 color=\"white\" if conf_matrix[i, j] > conf_matrix.max() / 2. else \"black\")\n\nplt.show()\n\n# Visualize the first two principal components using PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nplt.figure(figsize=(8, 6))\nfor i in range(len(classes)):\n    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], label=classes[i])\n\nplt.title('PCA of Iris dataset')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend()\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 1.00\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=646 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-3.png){width=674 height=523}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}