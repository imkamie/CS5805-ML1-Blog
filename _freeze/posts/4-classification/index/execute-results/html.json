{
  "hash": "99d51a0590a2046fa99c54d82b0e02a9",
  "result": {
    "markdown": "---\nexecute:\n  echo: fenced\ntitle: \"Classification\"\ntitle-block-banner: false\nauthor: Kamila Nurkhametova\n---\n\nClassification is a fundamental task in supervised machine learning, aiming to assign predefined labels or categories to input data based on their features. During the training phase, the algorithm learns patterns and relationships between input features and labels, enabling it to predict labels for new, unseen data. Let's delve into key concepts related to classification:\n\n## Key Concepts\n\n### Classes or Labels\nThese are the predefined categories that the algorithm assigns to input data. Binary classification involves two classes (e.g., spam or not spam), while multiclass classification deals with more than two classes.\n\n### Features\nFeatures are characteristics or attributes of input data used by the algorithm to make predictions. They can be numerical or categorical and are represented as input variables.\n\n### Training Data\nThe labeled dataset used to train the classification model, consisting of examples with input features and corresponding output labels.\n\n### Model\nThe learned representation of patterns and relationships in the training data, capable of making predictions on new, unseen data.\n\n### Prediction\nOnce trained, the model predicts class labels for new instances based on their input features.\n\n## Common Classification Algorithms\n\nVarious algorithms are employed for classification, each suited to different scenarios. Some common ones include logistic regression, decision trees, support vector machines, k-nearest neighbors, and neural networks. The choice depends on factors like data nature, complexity of relationships, and desired model interpretability.\n\n## Classification in Action\n\nNow, let's implement a simple classification example using the Iris dataset and the K-Nearest Neighbors (KNN) algorithm.\n\n::: {.cell execution_count=1}\n```` { .cell-code}\n```{{python}}\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.decomposition import PCA\n\n# Load the Iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Standardize features by removing the mean and scaling to unit variance\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Initialize and train the K-Nearest Neighbors classifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = knn.predict(X_test)\n\n# Calculate and print accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Visualize the confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8, 6))\nplt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion Matrix')\nplt.colorbar()\n\nclasses = iris.target_names\ntick_marks = np.arange(len(classes))\nplt.xticks(tick_marks, classes)\nplt.yticks(tick_marks, classes)\n\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\n\nfor i in range(len(classes)):\n    for j in range(len(classes)):\n        plt.text(j, i, format(conf_matrix[i, j], 'd'),\n                 ha=\"center\", va=\"center\",\n                 color=\"white\" if conf_matrix[i, j] > conf_matrix.max() / 2. else \"black\")\n\nplt.show()\n\n# Visualize the first two principal components using PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nplt.figure(figsize=(8, 6))\nfor i in range(len(classes)):\n    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], label=classes[i])\n\nplt.title('PCA of Iris dataset')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend()\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy: 1.00\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=646 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-3.png){width=674 height=523}\n:::\n:::\n\n\nIn this example, we use the Iris dataset, split it into training and testing sets, standardize features, and train a K-Nearest Neighbors classifier. We evaluate its performance using accuracy and visualize the confusion matrix and the first two principal components using PCA. The confusion matrix provides insights into the classifier's performance on different classes, while PCA visualization helps in understanding the data's structure in a lower-dimensional space.\n\n## Conclusion\n\nClassification is a powerful tool with applications ranging from spam detection to medical diagnosis. Understanding the underlying concepts and utilizing appropriate algorithms are essential for building accurate and effective models. Through continuous exploration and refinement, the field of classification continues to contribute significantly to the advancement of machine learning and data science.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}