{
  "hash": "441a745c9466371edbc86ee70d4916de",
  "result": {
    "markdown": "---\nexecute:\n  echo: fenced\ntitle: \"Clustering\"\ntitle-block-banner: false\nauthor: Kamila Nurkhametova\n---\n\nClustering, a cornerstone of unsupervised machine learning, empowers the discovery of inherent patterns and relationships within data without pre-existing labels. This versatile technique finds applications in diverse domains, including data analysis, customer segmentation, recommender systems, search engines, image segmentation, semi-supervised learning, and dimensionality reduction.\n\n## k-means: Partitioning Data with Centroids\n\nThe k-means algorithm, pioneered by Stuart Lloyd at Bell Labs in 1957, has evolved into a powerful clustering method. Although initially conceived for pulse-code modulation, its publication in 1982 marked its entry into broader usage. Sometimes referred to as the Lloydâ€“Forgy algorithm, its core principle involves partitioning data into k clusters based on similarity.\n\nBefore we delve into the implementation, let's import the necessary libraries for our clustering analysis.\n\n::: {.cell execution_count=1}\n```` { .cell-code}\n```{{python}}\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n```\n\n````\n:::\n\n\n**Determining Optimal Number of Clusters with the Elbow Method:**\nThe Elbow Method is a technique used to find the optimal number of clusters in a dataset. It involves running k-means clustering on the dataset for a range of values of k (number of clusters) and plotting the sum of squared distances from each point to its assigned center. The \"elbow\" of the curve represents the point where adding more clusters does not significantly improve the fit.\n\n::: {.cell execution_count=2}\n```` { .cell-code}\n```{{python}}\n# Load the wine dataset\ndata = load_wine()\nX = data.data\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Determine the optimal number of clusters using the Elbow Method\ndistortions = []\nfor i in range(1, 10):\n    kmeans = KMeans(n_clusters=i, n_init=10, random_state=42)\n    kmeans.fit(X)\n    distortions.append(kmeans.inertia_)\n\n# Plot the Elbow Curve\nplt.figure()\nplt.plot(range(1, 10), distortions, marker='o')\nplt.title('Elbow Method for Optimal k')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Distortion')\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=602 height=449}\n:::\n:::\n\n\nThe Elbow Method indicates that the optimal number of clusters for this dataset is 3.\nWe apply the k-means algorithm to the standardized dataset with three clusters. The resulting cluster labels are assigned to each data point. The scatter plot illustrates the data points in this reduced space, with each point colored according to its assigned cluster. The red markers represent the centroids of the clusters.\n\nThis visualization provides a clear representation of how the k-means algorithm has grouped the data into distinct clusters based on similarity.\n\n::: {.cell execution_count=3}\n```` { .cell-code}\n```{{python}}\n# Apply k-means clustering with k=3\nkmeans = KMeans(n_clusters=3, n_init='auto')\nkmeans.fit(X)\nlabels = kmeans.labels_\n\n# Reduce data to two dimensions using PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Visualize the clusters and centroids in 2D\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\ncenters_pca = pca.transform(kmeans.cluster_centers_)\nplt.scatter(centers_pca[:, 0], centers_pca[:, 1], s=200, c='red')\nplt.title('k-means Clustering')\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=569 height=431}\n:::\n:::\n\n\n## DBSCAN: Discovering Density-Based Clusters\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is an alternative approach to clustering that excels in identifying clusters with varying shapes and sizes.\n\nLet's explore the application of DBSCAN to our dataset. Before running DBSCAN, we standardize the features using the `StandardScaler`. The hyperparameters `eps` and `min_samples` control the density-based clustering. `eps` defines the maximum distance between two samples for one to be considered as in the neighborhood of the other, and `min_samples` sets the minimum number of samples required to form a dense region.\n\n::: {.cell execution_count=4}\n```` { .cell-code}\n```{{python}}\n# Standardize the features for DBSCAN\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply DBSCAN clustering\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nlabels = dbscan.fit_predict(X_scaled)\n\n# Reduce data to two dimensions using PCA\npca = PCA(n_components=2)\nclustered_data_2d = pca.fit_transform(X_scaled)\n\n# Visualize the clusters, highlighting noise points in red\nplt.scatter(clustered_data_2d[:, 0], clustered_data_2d[:, 1], c=labels, cmap='viridis')\ncenters_pca = pca.transform(kmeans.cluster_centers_)\nplt.scatter(centers_pca[:, 0], centers_pca[:, 1], s=200, c='red')\n\n# Add labels and title\nplt.title('DBSCAN Clustering')\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=569 height=431}\n:::\n:::\n\n\nUnlike k-means, DBSCAN can identify clusters of arbitrary shapes and is robust to outliers.\n\nThis visualization provides insights into the structure of the data, showcasing how DBSCAN has grouped points based on their density, thereby revealing clusters with varying shapes and sizes.\n\n## Conclusion\n\nk-means and DBSCAN serve as valuable tools in the clustering toolkit, offering distinct approaches to uncovering patterns and relationships in unlabeled data.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}