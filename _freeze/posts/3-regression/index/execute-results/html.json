{
  "hash": "c4e1403167a4d1fb0311a46bb1602bed",
  "result": {
    "markdown": "---\nexecute:\n  echo: fenced\ntitle: \"Linear and Nonlinear regression\"\ntitle-block-banner: false\nauthor: Kamila Nurkhametova\n---\n\nLinear and nonlinear regression are both techniques used in machine learning and statistics to model the relationship between independent variables (features) and a dependent variable (target). The primary difference between them lies in the nature of the relationship they can capture.\n\n## Linear Regression:\n\n**Nature of Relationship:**\nLinear regression assumes a linear relationship between the independent and dependent variables. This means that the relationship can be represented by a straight line.\n\n**Equation:**\nThe equation for a simple linear regression with one independent variable is: $y = mx + b$, where $y$ is the dependent variable, $x$ is the independent variable, $m$ is the slope, and $b$ is the y-intercept.\n\n**Optimization:**\nThe model parameters (slope and intercept) are typically estimated using methods like Ordinary Least Squares (OLS) to minimize the sum of squared differences between the observed and predicted values.\n\n## Nonlinear Regression:\n\n**Nature of Relationship:**\nNonlinear regression allows for more complex relationships between the independent and dependent variables. The relationship can take various forms, such as quadratic, exponential, logarithmic, or other nonlinear shapes.\n\n**Equation:**\nThe equation for a general nonlinear regression model is more complex and depends on the specific form of the relationship being modeled. For example, it could be $y = a \\cdot \\log(x) + b$.\n\n**Optimization:**\nOptimization techniques such as gradient descent or other nonlinear optimization methods are used to find the parameters that minimize the difference between the predicted and observed values.\n\n\nIn practice, the choice between linear and nonlinear regression depends on the characteristics of the data and the underlying assumptions about the relationship being modeled. Data exploration and validation techniques are essential in determining the most appropriate approach.\n\n## Example: Linear Regression with California Housing Data\n\nLet's illustrate linear regression using the California Housing dataset. We'll predict housing prices based on features and visualize the model's performance.\n\n::: {.cell execution_count=1}\n```` { .cell-code}\n```{{python}}\n# Import necessary libraries\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import fetch_california_housing\n\n# Load the California Housing dataset\ncalifornia_housing = fetch_california_housing()\nX = california_housing.data\ny = california_housing.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a linear regression model\nmodel = LinearRegression()\n\n# Train the model on the training set\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nprint(f'Mean Squared Error: {mse}')\n\n# Plotting the results\nplt.scatter(y_test, y_pred)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r-', lw=2)\nplt.xlabel(\"Actual Prices\")\nplt.ylabel(\"Predicted Prices\")\nplt.title(\"Actual Prices vs Predicted Prices\")\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 0.5558915986952422\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=585 height=449}\n:::\n:::\n\n\nIn the visualization, the red line represents a perfect prediction, and the scatter plot compares actual housing prices with the predicted values. The Mean Squared Error (MSE) quantifies the model's performance. Adjustments to the model or exploration of nonlinear regression may be necessary based on the analysis.\n\n## Conclusion\n\nLinear and nonlinear regression techniques offer versatile approaches for modeling relationships in data. Linear regression provides a simple and interpretable solution for linear dependencies, while nonlinear regression extends the capability to capture more complex patterns. The choice between them should be guided by the specific characteristics of the data and the nature of the relationship under investigation. Continuous evaluation and refinement of models based on performance metrics, such as Mean Squared Error, are crucial for building robust and accurate regression models.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}