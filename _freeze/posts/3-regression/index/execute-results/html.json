{
  "hash": "017a53442ed05d62231a8d14b1a159af",
  "result": {
    "markdown": "---\nexecute:\n  echo: fenced\ntitle: \"Linear and Nonlinear regression\"\ntoc: true\ntoc-title: \"Table of Contents\"\ntitle-block-banner: false\nauthor: Kamila Nurkhametova\n---\n\nLinear and nonlinear regression are both techniques used in machine learning and statistics to model the relationship between independent variables (features) and a dependent variable (target). The primary difference between them lies in the nature of the relationship they can capture.\n\n## Linear Regression:\n\n**Nature of Relationship:**\n   Linear regression assumes a linear relationship between the independent and dependent variables. This means that the relationship can be represented by a straight line.\n\n**Equation:**\n   The equation for a simple linear regression with one independent variable is: \\(y = mx + b\\), where \\(y\\) is the dependent variable, \\(x\\) is the independent variable, \\(m\\) is the slope, and \\(b\\) is the y-intercept.\n\n**Optimization:**\n   The model parameters (slope and intercept) are typically estimated using methods like Ordinary Least Squares (OLS) to minimize the sum of squared differences between the observed and predicted values.\n\n<!-- **Example:**\n   Predicting house prices based on the number of bedrooms. -->\n\n## Nonlinear Regression:\n\n**Nature of Relationship:**\n   Nonlinear regression allows for more complex relationships between the independent and dependent variables. The relationship can take various forms, such as quadratic, exponential, logarithmic, or other nonlinear shapes.\n\n**Equation:**\n   The equation for a general nonlinear regression model is more complex and depends on the specific form of the relationship being modeled. For example, it could be \\(y = a \\cdot \\log(x) + b\\).\n\n**Optimization:**\n   Optimization techniques such as gradient descent or other nonlinear optimization methods are used to find the parameters that minimize the difference between the predicted and observed values.\n\n<!-- **Example:** -->\n   <!-- Modeling the growth of a population over time using an exponential function. -->\n\nIn practice, the choice between linear and nonlinear regression depends on the characteristics of the data and the underlying assumptions about the relationship being modeled. Data exploration and validation techniques are essential in determining the most appropriate approach.\n\n::: {.cell execution_count=1}\n```` { .cell-code}\n```{{python}}\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import fetch_california_housing\n\n# Load the Boston Housing dataset\nboston = fetch_california_housing()\nX = boston.data\ny = boston.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create a linear regression model\nmodel = LinearRegression()\n\n# Train the model on the training set\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nprint(f'Mean Squared Error: {mse}')\nslope, intercept = np.polyfit(y_test, y_pred, 1)\nbest_fit_line = np.poly1d([slope, intercept])\n# Plotting the results\nplt.scatter(y_test, y_pred)\nplt.plot(y_test, best_fit_line(y_test), 'r-', lw=2)\nplt.xlabel(\"Actual Prices\")\nplt.ylabel(\"Predicted Prices\")\nplt.title(\"Actual Prices vs Predicted Prices\")\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 0.5558915986952422\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=585 height=449}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}