{
  "hash": "6cac4bd8251e1ec5f656ec430994e80b",
  "result": {
    "markdown": "---\nexecute:\n  echo: fenced\ntitle: \"Clustering\"\ntoc: true\ntoc-title: \"Table of Contents\"\ntitle-block-banner: false\n---\n\nClustering is a technique used in unsupervised machine learning to group similar instances together into clusters. The goal is to identify patterns and relationships within the data without any pre-existing labels. Clustering is a great tool for data analysis, customer segmentation, recommender systems, search engines, image segmentation, semi-supervised learning, dimensionality reduction, and more.\n\n\n## k-means\n\nIt was proposed by Stuart Lloyd at Bell Labs in 1957 as a technique for pulse-code modulation, but it was only published outside of the company in 1982.⁠ In 1965, Edward W. Forgy had published virtually the same algorithm, so k-means is sometimes referred to as the Lloyd–Forgy algorithm.\n\n::: {.cell execution_count=1}\n```` { .cell-code}\n```{{python}}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_wine\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n```\n\n````\n:::\n\n\n::: {.cell execution_count=2}\n```` { .cell-code}\n```{{python}}\n# Load the wine dataset\ndata = load_wine()\nX = data.data\n\n# Standardize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Calculate distortion for a range of cluster values\ndistortions = []\nfor i in range(1, 10):\n    kmeans = KMeans(n_clusters=i, n_init=10, random_state=42)\n    kmeans.fit(X)\n    distortions.append(kmeans.inertia_)\n\n# Plot the elbow curve\nplt.figure()\nplt.plot(range(1, 10), distortions, marker='o')\nplt.title('Elbow Method for Optimal k')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Distortion')\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=602 height=449}\n:::\n:::\n\n\nAs we can see, the best number of cluster is 3.\n\n::: {.cell execution_count=3}\n```` { .cell-code}\n```{{python}}\nkmeans = KMeans(n_clusters=3, n_init='auto')\nkmeans.fit(X)\nlabels = kmeans.labels_\n\n\n# Reduce the data to two dimensions using PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n\n# Plot the data\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n\n# Calculate the cluster centers in the PCA reduced space and plot them\ncenters_pca = pca.transform(kmeans.cluster_centers_)\nplt.scatter(centers_pca[:, 0], centers_pca[:, 1], s=200, c='red')\n\nplt.title('k-means Clustering')\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=569 height=431}\n:::\n:::\n\n\n## DBSCAN\n\n::: {.cell execution_count=4}\n```` { .cell-code}\n```{{python}}\n# Standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply DBSCAN clustering\neps_value = 0.5\nmin_samples_value = 5\ndbscan = DBSCAN(eps=eps_value, min_samples=min_samples_value)\nlabels = dbscan.fit_predict(X_scaled)\n\n# Reduce the data to two dimensions using PCA\npca = PCA(n_components=2)\nclustered_data_2d = pca.fit_transform(X_scaled)\n\n# Plot the data\nplt.scatter(clustered_data_2d[:, 0], clustered_data_2d[:, 1], c=labels, cmap='viridis')\n\n# Highlight noise points (label -1) with a different color\ncenters_pca = pca.transform(kmeans.cluster_centers_)\nplt.scatter(centers_pca[:, 0], centers_pca[:, 1], s=200, c='red')\n\n# Add labels and title\nplt.title('DBSCAN Clustering')\n\n# plt.legend()\nplt.show()\n```\n\n````\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=569 height=431}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}